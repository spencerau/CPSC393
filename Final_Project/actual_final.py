# -*- coding: utf-8 -*-
"""actual final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rrbk8B0YdC_h-J7X2RomD7LOiiFvfkh-
"""

# import os
# # Ensure kaggle folder exists
# os.makedirs('/root/.kaggle', exist_ok=True)

# # Assuming you've uploaded kaggle.json to your Colab working directory
# !mv kaggle.json /root/.kaggle/

# # This permissions change avoids a warning on Kaggle tool startup.
# !chmod 600 /root/.kaggle/kaggle.json



# # Download COCO 2017 dataset. Replace 'dataset-name' with actual dataset name
# !kaggle datasets download -d awsaf49/coco-2017-dataset

# !unzip -q coco-2017-dataset.zip -d /content/

import json
import numpy as np
import os
import tensorflow
import tensorflow as tf

from tensorflow.keras.models import load_model


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input

from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.layers import Input, LSTM, GRU, Embedding, Dense, Dropout, Add
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tqdm import tqdm
from tensorflow.keras.callbacks import EarlyStopping, CSVLogger

import random
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.meteor.meteor import Meteor


def load_annotations(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data

# Load annotations
train_annotations = load_annotations('coco2017/annotations/captions_train2017.json')
val_annotations = load_annotations('coco2017/annotations/captions_val2017.json')

# Create a mapping from image_id to filename for quick lookup
image_id_to_filename = {img['id']: img['file_name'] for img in train_annotations['images']}

# Initialize a dictionary to hold the captions mapped to image filenames
train_descriptions = {}
for ann in train_annotations['annotations']:
    image_id = ann['image_id']
    img_file_name = image_id_to_filename.get(image_id)
    if img_file_name:
        if img_file_name not in train_descriptions:
            train_descriptions[img_file_name] = []
        train_descriptions[img_file_name].append('<start> ' + ann['caption'] + ' <end>')

# Create a mapping from image_id to filename for quick lookup
val_image_id_to_filename = {img['id']: img['file_name'] for img in val_annotations['images']}

# Initialize a dictionary to hold the captions mapped to image filenames
val_descriptions = {}
for ann in val_annotations['annotations']:
    image_id = ann['image_id']
    img_file_name = val_image_id_to_filename.get(image_id)
    if img_file_name:
        if img_file_name not in val_descriptions:
            val_descriptions[img_file_name] = []
        val_descriptions[img_file_name].append('<start> ' + ann['caption'] + ' <end>')

# Prepare the base model
base_model = EfficientNetB0(include_top=False,
                            weights='imagenet',
                            pooling='avg')

base_model.trainable = False
inputs = tf.keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
model = tf.keras.Model(inputs, x)
model.summary()

def preprocess_image(file_path):
    # Read the image from the file
    img_raw = tf.io.read_file(file_path)
    # Decode the image
    img_tensor = tf.image.decode_jpeg(img_raw, channels=3)
    # Resize the image
    img_final = tf.image.resize(img_tensor, [224, 224])
    # Preprocess for model
    img_final = preprocess_input(img_final)
    return img_final

def extract_features(image_paths):
    # Create a dataset from image paths
    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)
    image_ds = path_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    image_ds = image_ds.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)

    # Extract features
    features = model.predict(image_ds, verbose=1)

    # Map filenames to their extracted features
    features_dict = {os.path.basename(path): feature for path, feature in zip(image_paths, features)}
    return features_dict


# For training images
train_image_directory = 'coco2017/train2017'
train_image_filenames = os.listdir(train_image_directory)
train_image_paths = [os.path.join(train_image_directory, filename) for filename in train_image_filenames]
# Extract features for training images
train_features = extract_features(train_image_paths)

# For validation images
val_image_directory = 'coco2017/val2017'
val_image_filenames = os.listdir(val_image_directory)
val_image_paths = [os.path.join(val_image_directory, filename) for filename in val_image_filenames]
# Extract features for validation images
val_features = extract_features(val_image_paths)

# Save validation features
np.save('val_features.npy', val_features)

# Save validation descriptions
with open('val_descriptions.json', 'w') as f:
    json.dump(val_descriptions, f)

# Step 1: Prepare text data for training and validation
all_captions = []

# Include training captions
for img_captions in train_descriptions.values():
    all_captions.extend(img_captions)

# Include validation captions
for img_captions in val_descriptions.values():
    all_captions.extend(img_captions)


# Tokenize captions
tokenizer = Tokenizer(num_words=15000, oov_token="<unk>")
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1

# Find the maximum length of any caption in the dataset
max_length = max(len(tokenizer.texts_to_sequences([c])[0]) for c in all_captions)

# save tokenizer
tokenizer_json = tokenizer.to_json()
with open('tokenizer.json', 'w') as f:
    f.write(tokenizer_json)
print("Tokenizer Saved")

# Step 2: Create data generator for fitting

def data_generator(descriptions, features, tokenizer, max_length, vocab_size, num_photos_per_batch):
    X1, X2, y = [], [], []
    n = 0

    while True:
        for key, desc_list in descriptions.items():
            if key in features:
                photo = features[key]
                for desc in desc_list:
                    seq = tokenizer.texts_to_sequences([desc])[0]
                    for i in range(1, len(seq)):
                        in_seq, out_seq = seq[:i], seq[i]
                        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]
                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
                        X1.append(photo)
                        X2.append(in_seq)
                        y.append(out_seq)

                n += 1
                if n >= num_photos_per_batch:
                    #print(f"Batch shapes: X1: {np.array(X1).shape}, X2: {np.array(X2).shape}, y: {np.array(y).shape}")  # Add this line
                    yield ([np.array(X1), np.array(X2)], np.array(y))
                    X1, X2, y = [], [], []
                    n = 0
            else:
              break
              #print(f"Warning: Key '{key}' not found in features.")

# Simplified Model Architecture
input1 = Input(shape=(1280,))
#print(input1.shape())
#input1.squeeze()
#print(input1.shape())
fe1 = Dropout(0.5)(input1)
fe2 = Dense(128, activation='relu')(fe1)  # Smaller dense layer

input2 = Input(shape=(max_length,))
#input2.squeeze()
#print(input2.shape())
se1 = Embedding(vocab_size, 128, mask_zero=True)(input2)  # Smaller embedding size
se2 = Dropout(0.5)(se1)
se3 = LSTM(128, return_sequences=False)(se2)  # Only return the last output

decoder1 = Add()([fe2, se3])
decoder2 = Dense(128, activation='relu')(decoder1)  # Further reduction
outputs = Dense(vocab_size, activation='softmax')(decoder2)

print("Original Shape of input1:", input1.shape)
print("Original Shape of input2:", input2.shape)

# model = Model(inputs=[input1, input2], outputs=outputs)



# model.compile(loss='categorical_crossentropy',
#               optimizer='adam',
#               metrics=['accuracy'])

# Step 4: Train the model
# Assuming batch_size and other parameters are defined elsewhere in your code
batch_size = 10 # Example batch size
vocab_size = vocab_size  # Update this based on your tokenizer's vocabulary size
max_length = 52  # Update this based on the maximum length of descriptions

# Training generator
train_generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size, batch_size)
# Validation generator
val_generator = data_generator(val_descriptions, val_features, tokenizer, max_length, vocab_size, batch_size)

steps_per_epoch = len(train_descriptions) // batch_size
validation_steps = len(val_descriptions) // batch_size
# Wrap your existing generator


# monitor: This is the metric to be monitored, in this case, 'val_accuracy'.
# patience: Number of epochs with no improvement after which training will be stopped.
# verbose: If 1, prints out messages for stopping.
# mode: In 'max' mode, training will stop when the quantity monitored has stopped increasing.
# restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity.
early_stopping = EarlyStopping(monitor='val_accuracy',
                               patience=5,
                               verbose=1,
                               mode='max',
                               restore_best_weights=True)

csv_logger = CSVLogger('training_log.csv', append=False, separator=';')

# model.summary()

# # Set the device to GPU device 7
# with tf.device('/device:GPU:7'):

#     model.fit(x=train_generator,
#             epochs=100,
#             steps_per_epoch=steps_per_epoch,
#             validation_data=val_generator,
#             validation_steps=validation_steps,
#             callbacks=[early_stopping, csv_logger])

# #model.summary()

# model.save('caption_model')

# Load the pre-trained model
model = load_model('caption_model')

# def generate_caption(image, model, tokenizer, max_length):
#     # Start the sequence of input for the RNN with the start token
#     in_text = '<start>'
#     for _ in range(max_length):
#         # Convert the current sequence of words to a sequence of indices
#         sequence = tokenizer.texts_to_sequences([in_text])[0]
#         # Pad the input sequence to the fixed length
#         sequence = pad_sequences([sequence], maxlen=max_length)

#         yhat = model.predict([np.array([image]), sequence], verbose=0)
#         # Convert the probability distribution to a single integer (word index)
#         yhat = np.argmax(yhat)
#         # Look up the word index to get the corresponding word
#         word = tokenizer.index_word.get(yhat, None)
#         # Break if the model predicts the end token or no word could be found
#         if word is None or word == 'end':
#             break
#         # Append the word to the current sequence and continue
#         in_text += ' ' + word
#     # Remove the start token from the generated caption
#     final_caption = in_text.split(' ', 1)[1] if len(in_text.split(' ', 1)) > 1 else in_text
#     return final_caption

# Function to generate caption
def generate_caption(image, model, tokenizer, max_length):
    in_text = '<start>'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([np.array([image]), sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = tokenizer.index_word.get(yhat, None)
        if word is None or word == 'end':
            break
        in_text += ' ' + word
    final_caption = in_text.split(' ', 1)[1] if len(in_text.split(' ', 1)) > 1 else in_text
    return final_caption

# sample_keys = random.sample(list(val_features.keys()), 3)
# for idx, img_key in enumerate(sample_keys):
#     fig, ax = plt.subplots(figsize=(6, 4))  # Adjust figsize for individual image visibility
#     # Load image
#     img_path = f'coco2017/val2017/{img_key}'  # Update with the actual path to your images
#     image = plt.imread(img_path)
#     # Generate caption
#     caption = generate_caption(val_features[img_key], model, tokenizer, max_length)
#     # Display image
#     ax.imshow(image)
#     ax.axis('off')  # Hide the axes

#     # Add caption as text with background
#     ax.add_patch(Rectangle((0, 0.85), 1, 0.15, color='black', alpha=0.5, transform=ax.transAxes))
#     ax.text(0.5, 0.9, caption, fontsize=9, color='white', ha='center', va='center', transform=ax.transAxes)

#     # Save the figure
#     plt.savefig(f'output_{img_key}.png', bbox_inches='tight')  # Save as PNG
#     plt.close(fig)  # Close the plot to free memory


# Evaluation Metrics with progress bar
def evaluate_model(model, descriptions, features, tokenizer, max_length):
    actual, predicted = list(), list()
    for key, desc_list in tqdm(descriptions.items(), desc="Generating Captions"):
        if key in features:
            y_pred = generate_caption(features[key], model, tokenizer, max_length)
            references = [d.split() for d in desc_list]
            actual.append(references)
            predicted.append(y_pred.split())
    return actual, predicted

# Calculate BLEU, CIDEr, and METEOR scores
def calculate_scores(actual, predicted):
    smoothie = SmoothingFunction().method4
    print("Computing BLEU Score")
    bleu_score = corpus_bleu(actual, predicted, smoothing_function=smoothie)
    print(f'BLEU Score: {bleu_score}')
    
    print("Computing CIDEr Score")
    cider = Cider()
    cider_score, _ = cider.compute_score({i: [' '.join(a[0])] for i, a in enumerate(actual)}, 
                                         {i: [' '.join(p)] for i, p in enumerate(predicted)})
    print(f'CIDEr Score: {cider_score}')
    
    # print("Computing METEOR Score")
    # meteor = Meteor()
    # meteor_score, _ = meteor.compute_score({i: [' '.join(a[0])] for i, a in enumerate(actual)}, 
    #                                        {i: [' '.join(p)] for i, p in enumerate(predicted)})
    # print(f'METEOR Score: {meteor}')
    
    return bleu_score, cider_score#, meteor_score


# Evaluate model with progress bar
print("Evaluating model...")
actual, predicted = evaluate_model(model, val_descriptions, val_features, tokenizer, max_length)

# Calculate scores
print("Calculating scores...")
bleu, cider = calculate_scores(actual, predicted)

print(f'BLEU Score: {bleu}')
print(f'CIDEr Score: {cider}')
#print(f'METEOR Score: {meteor}')