# -*- coding: utf-8 -*-
"""Copy of HW4_SP24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OOcqTG_-JKHXwQoWkksqUJO_tu5ARQ7v

# Homework 4 (Sequential Models)

1. Choose a book or other text to train your model on (I suggest [Project Gutenberg](https://www.gutenberg.org/ebooks/) to find .txt files but you can find them elsewhere). Make sure your file is a `.txt` file. Clean your data. Build a sequential model (LSTM, GRU, SimpleRNN, or Transformer) that generates new lines based on the training data (NO TRANSFER LEARNING). While your model doesn't need to have perfect accuracy, you must have an appropriate architecture and train it for a reasonable amount of epochs.

Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://colab.research.google.com/drive/12rxdjlEA9JOMQ_jioiEmwjDf6iPhsMmx?usp=sharing) as a reference for how to generate text from a trained model). Assess in detail how good they are, what they're good at, what they struggle to do well. INCLUDE THESE 10 sequences in your report.

2. Make a new model with ONE substantial adjustment (e.g. use a custom embedding layer if you didn't already, use a pre-trained embedding layer if you didn't already, use a DEEP LSTM/GRU with multiple recurrent layers, use a pre-trained model to do transfer learning and fine-tune it...etc.). While your model doesn't need to have perfect accuracy, you must have an appropriate architecture and train it for a reasonable amount of epochs.

Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://colab.research.google.com/drive/12rxdjlEA9JOMQ_jioiEmwjDf6iPhsMmx?usp=sharing) as a reference for how to generate text from a trained model. INCLUDE THESE 10 sequences in your report. Assess in detail how good they are, what they're good at, what they struggle to do well.  Did the performance of your model change?

3. Then create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section.
"""

import numpy as np
import tensorflow as tf
import pathlib
import json
import os

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import Input
from tensorflow.keras import Model
from keras.regularizers import L1L2
from tensorflow.keras.callbacks import EarlyStopping


os.environ["CUDA_VISIBLE_DEVICES"] = "7"
gpu_device = '/device:GPU:7'


# load ascii text and covert to lowercase
filename = "Brave_New_World_Aldous_Huxley_djvu.txt"
raw_text = open(filename, 'r', encoding='utf-8').read()
raw_text = raw_text.lower()

raw_text[0:100]

# create mapping of unique chars to integers
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))

n_chars = len(raw_text)
n_vocab = len(chars)
print("Total Characters: ", n_chars)
print("Total Vocab: ", n_vocab)

# prepare the dataset of input to output pairs encoded as integers
seq_length = 100 # 100 characters as input
dataX = []
dataY = []
for i in range(0, n_chars - seq_length, 1):
 seq_in = raw_text[i:i + seq_length] # generate 100 character input
 seq_out = raw_text[i + seq_length] # grab next character

 dataX.append([char_to_int[char] for char in seq_in])
 dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)
print("Total Patterns: ", n_patterns)

# reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (n_patterns, seq_length, 1))
# normalize
X = X / float(n_vocab)
# one hot encode the output variable
y = to_categorical(dataY)
print(y[0])

def save_model(model, export_dir):
    # Save the TensorFlow model in a Keras-compatible format
    model.save(export_dir)

    converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
    tflite_model = converter.convert()
    tflite_model_file = pathlib.Path('./a4_model.tflite')
    tflite_model_file.write_bytes(tflite_model)

# LSTM model
inputs = Input(shape = (X.shape[1], X.shape[2]))
x = LSTM(256, return_sequences=True)(inputs)
x = LSTM(256)(x)
x = Dropout(0.2)(x)
output = Dense(y.shape[1], activation='softmax')(x)

model = Model(inputs = inputs, outputs = output)

model.summary()

def train_model():
    model.compile(optimizer = "adam",
                    loss='categorical_crossentropy',
                    metrics=['accuracy'])

    # Set up EarlyStopping callback
    early_stopping = EarlyStopping(
        monitor = 'loss',
        patience = 10,
        restore_best_weights = True
    )

    # Train the model with the data generators and the EarlyStopping callback
    history = model.fit(
        X, y,
        epochs = 150,
        batch_size = 128,
        callbacks = [early_stopping],
        verbose = 1
    )

    return history

with tf.device(gpu_device):
    history = train_model()

    history_path = 'history/training_history.json' 
    with open(history_path, 'w') as f:
        json.dump(history.history, f, indent=4)

    print(f"Training history saved to {history_path}")

    save_model(model, "a4_model")

    print("\nModel trained and saved.\n")

