# -*- coding: utf-8 -*-
"""Copy of HW4_SP24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OOcqTG_-JKHXwQoWkksqUJO_tu5ARQ7v

# Homework 4 (Sequential Models)

2. Make a new model with ONE substantial adjustment (e.g. use a custom embedding layer if you didn't already, use a pre-trained embedding layer if you didn't already, use a DEEP LSTM/GRU with multiple recurrent layers, use a pre-trained model to do transfer learning and fine-tune it...etc.). While your model doesn't need to have perfect accuracy, you must have an appropriate architecture and train it for a reasonable amount of epochs.

Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://colab.research.google.com/drive/12rxdjlEA9JOMQ_jioiEmwjDf6iPhsMmx?usp=sharing) as a reference for how to generate text from a trained model. INCLUDE THESE 10 sequences in your report. Assess in detail how good they are, what they're good at, what they struggle to do well.  Did the performance of your model change?

3. Then create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section.
"""

import numpy as np
import tensorflow as tf
import pathlib
import json
import os
import torch

from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, EarlyStoppingCallback
from torch.utils.data import Dataset

# need to implement GPT2 Dataset Custom Class 
#Trainer expects the dataset to be an instance of a class that implements the __getitem__ and __len__ methods properly, 
# handling the data fetching based on the indices it provides during the training loop. The data should be formatted 
# in a way that each item fetched by the __getitem__ method returns a dictionary with the expected keys

class GPT2Dataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        # Retrieve the data for the specified index
        # Clone and detach tensors to avoid unnecessary tensor creation and ensure
        # that the graph history does not interfere with gradient calculations
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        # Setting labels for causal language modeling
        item['labels'] = item['input_ids'].clone()  # Assuming causal LM where labels are the input_ids
        return item


os.environ["CUDA_VISIBLE_DEVICES"] = "6"
gpu_device = '/device:GPU:6'

# Load pre-trained model
model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Use EOS token as padding in GPT-2
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenize and prepare dataset
filename = "Brave_New_World_Aldous_Huxley_djvu.txt"
with open(filename, 'r', encoding='utf-8') as file:
    text = file.read()

# Tokenize Text
encoded_inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt")

# Initialize the dataset
dataset = GPT2Dataset(encoded_inputs)


# Define training arguments
training_args = TrainingArguments(
    output_dir='finetune',           # output directory
    num_train_epochs=100,            # number of training epochs
    #metrics = ["accuracy"],
    per_device_train_batch_size=2,   # batch size for training
    per_device_eval_batch_size=2,    # batch size for evaluation
)


with tf.device(gpu_device):

    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
    )

    # Train the model
    trainer.train()

    # Save the model
    model.save_pretrained("./trained_gpt2")

    # Save history
    with open('history/finetune_history.json', 'w') as f:
        json.dump(trainer.state.log_history, f, indent=4)

    prompts = [
        "Describe your vision of a perfect society. How would it function day to day?",
        "What are the potential dangers of a society that prioritizes collective happiness over individual freedoms?",
        "Imagine a world where governments use technology to control thoughts. How would people react?",
        "Discuss the impact of universal happiness on creativity and innovation in society.",
        "What role should government have in the personal lives of its citizens in an ideal world?",
        "Debate the ethical implications of genetic engineering in humans to achieve societal goals.",
        "How would a society without any monetary system operate? Describe its economy and daily interactions.",
        "Envision a political system that perfectly balances individual liberty with societal order.",
        "What are the consequences of abolishing all forms of privacy to ensure safety and harmony?",
        "Describe a future where artificial intelligence governs society. What are the pros and cons?"
    ]

    # Open a file to write the outputs
    with open('finetune_output.txt', 'w') as f:
        for prompt_number, prompt in enumerate(prompts):
            
            # Encode the current prompt to tensor
            inputs = tokenizer.encode(prompt, return_tensors="pt")

            # Generate text using beam search
            outputs = model.generate(
                inputs,
                max_length=150,
                num_return_sequences=1, # one response per prompt
                num_beams=10,
                no_repeat_ngram_size=2,
                early_stopping=True
            )

            # Decode and process each output
            for i, output in enumerate(outputs):
                text = tokenizer.decode(output, skip_special_tokens=True)
                print(f"Generated {prompt_number+1}: {text}\n")
                f.write(f"Generated {prompt_number+1}: {text}\n\n")  # Ensure each entry


    # inputs = tokenizer.encode("Tell me about the ideal society", 
    #                         return_tensors="pt")

    # # temperature: Higher values result in more random completions.
    # # top_k: Limits the number of highest probability vocabulary tokens to be considered for each step; 
    # # setting it to a value like 40 or 50 can help with diversity.
    # # top_p: Uses nucleus sampling, where the top tokens are chosen such that they have a cumulative probability 
    # # of top_p; setting this to values like 0.92 can help control repetitiveness while maintaining sensible output.

    # # Generate outputs using beam search
    # outputs = model.generate(
    #     inputs,
    #     max_length=150,
    #     num_return_sequences=10,
    #     num_beams=10,  # Set the number of beams for beam search
    #     no_repeat_ngram_size=2,  # Prevents the model from repeating n-grams, helping to avoid repetition
    #     early_stopping=True,  # Optional: stops the beam search when all beams generate an EOS token
    # )

    # # Open a file and write the outputs
    # with open('finetune_output.txt', 'w') as f:
    #     for i, output in enumerate(outputs):
    #         text = tokenizer.decode(output, skip_special_tokens=True)
    #         print(f"Generated {i+1}: {text}\n")
    #         f.write(f"Generated {i+1}: {text}\n")


    # # Generating text
    # inputs = tokenizer.encode("Sample prompt to generate text", return_tensors="pt")
    # outputs = model.generate(inputs, max_length=50, num_return_sequences=10)
    # with open('finetune_output.txt', 'w') as f:
    #     for i, output in enumerate(outputs):
    #         text = tokenizer.decode(output, skip_special_tokens=True)  # Decode each output individually
    #         print(f"Generated {i+1}: {text}")
    #         f.write(f"Generated {i+1}: {text}\n")

