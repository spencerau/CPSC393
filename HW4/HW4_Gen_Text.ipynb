{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVDetIYKhvs9"
      },
      "source": [
        "# Homework 4 (Sequential Models)\n",
        "\n",
        "1. Choose a book or other text to train your model on (I suggest [Project Gutenberg](https://www.gutenberg.org/ebooks/) to find .txt files but you can find them elsewhere). Make sure your file is a `.txt` file. Clean your data. Build a sequential model (LSTM, GRU, SimpleRNN, or Transformer) that generates new lines based on the training data (NO TRANSFER LEARNING). While your model doesn't need to have perfect accuracy, you must have an appropriate architecture and train it for a reasonable amount of epochs.\n",
        "\n",
        "Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://colab.research.google.com/drive/12rxdjlEA9JOMQ_jioiEmwjDf6iPhsMmx?usp=sharing) as a reference for how to generate text from a trained model). Assess in detail how good they are, what they're good at, what they struggle to do well. INCLUDE THESE 10 sequences in your report.\n",
        "\n",
        "2. Make a new model with ONE substantial adjustment (e.g. use a custom embedding layer if you didn't already, use a pre-trained embedding layer if you didn't already, use a DEEP LSTM/GRU with multiple recurrent layers, use a pre-trained model to do transfer learning and fine-tune it...etc.). While your model doesn't need to have perfect accuracy, you must have an appropriate architecture and train it for a reasonable amount of epochs.\n",
        "\n",
        "Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://colab.research.google.com/drive/12rxdjlEA9JOMQ_jioiEmwjDf6iPhsMmx?usp=sharing) as a reference for how to generate text from a trained model. INCLUDE THESE 10 sequences in your report. Assess in detail how good they are, what they're good at, what they struggle to do well.  Did the performance of your model change?\n",
        "\n",
        "3. Then create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import keras as kb\n",
        "\n",
        "import string\n",
        "from random import randint\n",
        "from pickle import load\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "t15b7GYGiG9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "# use this?\n",
        "filename = \"Brave_New_World_Aldous_Huxley_djvu.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "raw_text[0:100]\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100 # 100 characters as input\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length] # generate 100 character input\n",
        " seq_out = raw_text[i + seq_length] # grab next character\n",
        "\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)\n",
        "print(y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nevyAquUd0Q",
        "outputId": "3550faa8-9d79-45da-e889-2b68d0c720a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  385183\n",
            "Total Vocab:  54\n",
            "Total Patterns:  385083\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip\n",
        "!unzip a4_model.zip\n",
        "\n",
        "# loads model from files\n",
        "model = tf.keras.models.load_model('./a4_model/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "czlAkIKeM8Xe",
        "outputId": "53e15035-66ab-4fae-96ac-b93befd580be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open a4_model.zip, a4_model.zip.zip or a4_model.zip.ZIP.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at ./a4_model/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-191e0fb069a8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loads model from files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./a4_model/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at ./a4_model/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "51O9BSCSUltP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "n_chars = 100\n",
        "\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(n_chars):\n",
        " x = np.reshape(pattern, (1, len(pattern), 1))\n",
        " x = x / float(n_vocab)\n",
        "\n",
        " # get model's prediction (as a one hot vec)\n",
        " prediction = model.predict(x, verbose=0) # predicted probs\n",
        " index = np.argmax(prediction) # find highest prop\n",
        "\n",
        " # use one hots to grab actual characters\n",
        " result = int_to_char[index]\n",
        "\n",
        " # string sequence together\n",
        " seq_in = [int_to_char[value] for value in pattern]\n",
        "\n",
        " # write sequence to console\n",
        " sys.stdout.write(result)\n",
        "\n",
        " # store pattern\n",
        " pattern.append(index)\n",
        " pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\n------------------------------------------------------------\")\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btP6x7TzUov_",
        "outputId": "fae3f100-d498-4a5b-c397-2d84ca8f384d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" act that it carries a bonus amounting to six months' \n",
            "salary\"; continued with some account of the te \"\n",
            "------------------------------------------------------------\n",
            "reen he had been his \n",
            "mind seasler and the savage was all and the savage was all out a \n",
            "sale continu\n",
            "------------------------------------------------------------\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_text(model, dataX, int_to_char, n_vocab, num_sequences, sequence_length=100):\n",
        "    for _ in range(num_sequences):\n",
        "        start = np.random.randint(0, len(dataX) - 1)\n",
        "        pattern = dataX[start]\n",
        "        print(\"Seed:\")\n",
        "        print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "        print(\"------------------------------------------------------------\")\n",
        "        sys.stdout.write(\"\\\"\")\n",
        "\n",
        "        output_text = []\n",
        "        # Generate characters\n",
        "        for i in range(sequence_length):\n",
        "            x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "            x = x / float(n_vocab)\n",
        "            prediction = model.predict(x, verbose=0)  # predicted probabilities\n",
        "            index = np.argmax(prediction)  # find highest prop\n",
        "            result = int_to_char[index]\n",
        "            output_text.append(result)\n",
        "            # Append to pattern for next prediction\n",
        "            pattern.append(index)\n",
        "            pattern = pattern[1:len(pattern)]\n",
        "\n",
        "        # Join all characters and split into words\n",
        "        full_text = ''.join(output_text).split()\n",
        "        print(\" \".join(full_text))\n",
        "        print(\"\\n------------------------------------------------------------\")\n",
        "    print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "30xb9yB6VM8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, dataX, int_to_char, n_vocab, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcNz98BHVOZP",
        "outputId": "254f0936-44a6-42be-c102-be04d7a5039b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" irst used officially in \n",
            "a.f. 214. why not before? two reasons, (a) ...\" \n",
            "\n",
            "\"these early experimenter \"\n",
            "------------------------------------------------------------\n",
            "\",\" said the savage, with a san and a perfect ce- cal, they had so hand and surned away and was some\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\"  and bacteriological conditioning of the embryo. practical instruc- \n",
            "tions for beta embryo-store wor \"\n",
            "------------------------------------------------------------\n",
            "\"k and the savage was a mong chain, the sruare was a prison of the south of the souther- the savage\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" . linda and he-linda was his \n",
            "mother (the word made lenina look uncomfortable)-were strangers in \n",
            "th \"\n",
            "------------------------------------------------------------\n",
            "\"e seventeen thousand as the savage was all all the savage reserval the savage round his hand and th\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" dictions. the land wasn't properly worked; there were \n",
            "strikes in all the factories; the laws were s \"\n",
            "------------------------------------------------------------\n",
            "\"eened to himself and shouted into the changed and startled and she had been the sen much and sent\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" ir hung down in grey wisps round his face. his body was \n",
            "bent and emaciated to the bone, almost fles \"\n",
            "------------------------------------------------------------\n",
            "\"hed and still said and the last of the south of the soof of the south of the sooi and a half and s\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" state about having a girl-it \n",
            "seemed rather ridiculous. but, taken detail by verbal detail, what a s \"\n",
            "------------------------------------------------------------\n",
            "\"u- pere pueer was a mong alons and the savage rooe and surnee off. \"but it sane to have the same\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" hought; and then (with what derisive ferocity!): \n",
            "\"sons eso tse-na.\" and he spat on the ground, as p \"\n",
            "------------------------------------------------------------\n",
            "\"nee and surong and suill srickled in the south of the south of the sooe of the south- the savage w\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" the \n",
            "women to whom he made proposals, the practical joking of his equals \n",
            "among the men. the mockery \"\n",
            "------------------------------------------------------------\n",
            "\"of the controller she was a sall to the mortin of the south of the sooa of the room of the reserva\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\"  eyes. \n",
            "\n",
            "\"my young friend,\" said the arch-community-songster in a tone of \n",
            "loud and solemn severity; \"\n",
            "------------------------------------------------------------\n",
            "\"but the sears of the controller she was on the forreritime that had been a cettain so and all the\n",
            "\n",
            "------------------------------------------------------------\n",
            "Seed:\n",
            "\" s in \n",
            "some indescribably delicious all-singing feely; where the dripping \n",
            "patchouli was more than sc \"\n",
            "------------------------------------------------------------\n",
            "\"tial of the souther- the savage was and sureaming and surned of the room and staring and surned and\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}