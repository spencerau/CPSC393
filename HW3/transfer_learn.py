# -*- coding: utf-8 -*-
"""Copy of HW3_SP24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DbZMv14JojEkuLj6vLHbsZ1mfVUZSjPL
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import os
import pathlib
import json

import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Corrected import for VGG16 and preprocess_input
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.regularizers import l2
from tensorflow.keras.losses import CategoricalCrossentropy

from tensorflow.keras.callbacks import EarlyStopping

# # make kaggle directory
# !mkdir ~/.kaggle

# # move kaggle.json to hidden kaggle folder
# !cp kaggle.json ~/.kaggle/

# # change permissions on file
# !chmod 600 ~/.kaggle/kaggle.json

# # download zipped data
# !kaggle datasets download -d kritikseth/fruit-and-vegetable-image-recognition

# !unzip -qq /content/fruit-and-vegetable-image-recognition.zip

os.environ["CUDA_VISIBLE_DEVICES"] = "7"
gpu_device = '/device:GPU:7'

model_filepath = "a3_model_finetune"

train_dir = "archive/train"
test_dir = "archive/test"
val_dir = "archive/validation"

# Function to count the number of images in each category directory
# stuff for EDA
def count_images(directory):
    categories = []
    counts = []
    # Iterate through each category directory
    for category in sorted(os.listdir(directory)):
        # Full path to the category directory
        category_path = os.path.join(directory, category)
        # Check if it's a directory
        if os.path.isdir(category_path):
            # Append category name and count of images
            categories.append(category)
            counts.append(len(os.listdir(category_path)))
    return categories, counts

# Get counts for each dataset
train_categories, train_counts = count_images(train_dir)
val_categories, val_counts = count_images(val_dir)
test_categories, test_counts = count_images(test_dir)

# # Plotting function
# def plot_counts(categories, counts, dataset_name):
#     plt.figure(figsize=(10, 8))
#     plt.barh(categories, counts, color='skyblue')
#     plt.xlabel('Number of Images')
#     plt.title(f'Number of Images per Category in the {dataset_name} Set')
#     plt.tight_layout()
#     plt.show()

# # Plot the counts for each dataset
# plot_counts(train_categories, train_counts, 'Train')
# plot_counts(val_categories, val_counts, 'Validation')
# plot_counts(test_categories, test_counts, 'Test')

def setup_data_generators(train_dir, val_dir, test_dir, batch_size=32):
    # Only augment training data with various image manipulation operations
    train_datagen = ImageDataGenerator(
        # Use VGG16's preprocess_input
        preprocessing_function=preprocess_input,
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    # The preprocess_input function standardizes images in a way that matches the original preprocessing applied to the images when the VGG16 model was trained.
    # This includes color channel normalization based on the means of the ImageNet dataset, which is crucial for models pre-trained on ImageNet.
    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical'
    )

    validation_generator = val_datagen.flow_from_directory(
        val_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical'
    )

    test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical'
    )

    return train_generator, validation_generator, test_generator

with tf.device(gpu_device):
    train_generator, validation_generator, test_generator = setup_data_generators(train_dir, val_dir, test_dir)

# The number of classes would be the length of the class_indices dictionary
num_classes = len(train_generator.class_indices)

print(f"There are {num_classes} classes in the dataset.")

"""# Homework 3 (Convolutional Neural Networks)

Choose a dataset that you're interested in from among these options (or choose your own data set as long as it's large enough and **you check with me** in advance to make sure it'll work):

- [Grape Disease Detection Data](https://www.kaggle.com/datasets/rm1000/augmented-grape-disease-detection-dataset)
- [Indian Bird Data](https://www.kaggle.com/datasets/arjunbasandrai/25-indian-bird-species-with-226k-images)
- [Skin Cancer Classification](https://www.kaggle.com/datasets/kylegraupe/skin-cancer-binary-classification-dataset)
- [Fruit and Veg Detection Data](https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition)
- [Large Scale Fish Data](https://www.kaggle.com/datasets/crowww/a-large-scale-fish-dataset)
- [Berkeley Segmentation Data](https://www.kaggle.com/datasets/balraj98/berkeley-segmentation-dataset-500-bsds500)

1. Then Build a Deep CONVOLUTIONAL Neural Network (No Recurrent Layers, no Transfer Learning, no Generative Models) using keras/tensorflow (at least 5 Convolutional Layers, and at least 3 Pooling Layers) to do one of the following tasks:

- Classify Images (e.g. Hot Dog vs. Not a Hot Dog)
- Compress Images (e.g. with a Convolutional AutoEncoder)
- Detect/Segment Objects (e.g. what pixels in the image contain a cat?)

Make sure that:

- your NN has some sort of regularization (or multiple types if needed)
- you've properly formatted and inputted your data into the network
- your model architechture and loss function are appropriate for the problem
- you print out at least 2 metrics for both train and test data to examine

2. Then Build a **Transfer Learning** Model that performs the same task. You can do Finetuning, or just add your own head to the pre-trained model. A list of pre-trained models available in Keras is [here](https://keras.io/api/applications/), however you're welcome to use any pre-trained model you'd like for the task, as long as you're implementing it through python code.

Make sure that:

- you've properly formatted and inputted your data into the network
- your model architechture and loss function are appropriate for the problem
- you print out at least 2 metrics for both train and test data to examine

Then create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section.
"""

# Load VGG16 pre-trained on ImageNet without the top layer (classifier)
base_model = VGG16(weights='imagenet', 
                    include_top=False, 
                    input_shape=(224, 224, 3))

# Freeze the convolutional base
base_model.trainable = False

# Create a custom classifier as the new top layer
# dropout: increase to lower overfitting, decrease for better learning 
# 0.2 = 20% of the input units are randomly excluded from each update cycle.
classifier = Sequential([
    Flatten(),
    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.4),
    Dense(36, activation='softmax')
])
# Stack the base model and the custom classifier
model = Sequential([
    base_model, 
    classifier
])

model.summary()

def train_model():
    model.compile(optimizer = "adam",
                    loss='categorical_crossentropy',
                    metrics=['accuracy', 'top_k_categorical_accuracy'])

    # Set up EarlyStopping callback
    early_stopping = EarlyStopping(
        monitor = 'val_loss',
        patience = 10,
        restore_best_weights = True
    )

    # Train the model with the data generators and the EarlyStopping callback
    history = model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs = 150,
        callbacks = [early_stopping],
        verbose = 1
    )

    return history

def unfreeze_model_layers(model, num_layers_to_unfreeze):
    # Unfreeze the top `num_layers_to_unfreeze` layers of the model
    for layer in model.layers[-num_layers_to_unfreeze:]:
        layer.trainable = True

def save_model(model, export_dir):
    # Save the TensorFlow model in a Keras-compatible format
    model.save(export_dir)

    converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
    tflite_model = converter.convert()
    tflite_model_file = pathlib.Path('./a3_model_finetune.tflite')
    tflite_model_file.write_bytes(tflite_model)

with tf.device(gpu_device):
    # Check if model already exists
    if not os.path.exists(model_filepath):
        history = train_model()

        history_path = 'history/training_history_unfreeze_base.json' 
        with open(history_path, 'w') as f:
            json.dump(history.history, f, indent=4)

        save_model(model, "a3_model_finetune")

        # # After training, evaluate the model on the test data
        # test_loss, test_accuracy, test_top_k_accuracy = model.evaluate(
        #     test_generator,
        #     verbose=1
        # )

        # # Print the evaluation results
        # print(f"Test Loss: {test_loss}")
        # print(f"Test Accuracy: {test_accuracy}")
        # print(f"Test Top K Accuracy: {test_top_k_accuracy}")
    
    else:
        print("\nModel already exists. Loading model for fine-tuning or evaluation.\n")

        print("\nFirst Time Fine Tuning; Unfreezing 3 Layers\n")
        model = tf.keras.models.load_model(model_filepath)
        # layers_unfreeze = 3
        
        unfreeze_model_layers(model, num_layers_to_unfreeze = 3)

        history = train_model()

        history_path = 'history/training_history_unfreeze_3.json' 
        with open(history_path, 'w') as f:
            json.dump(history.history, f, indent=4)

        save_model(model, "a3_model_finetune")

        # After training, evaluate the model on the test data
        test_loss, test_accuracy, test_top_k_accuracy = model.evaluate(
            test_generator,
            verbose=1
        )

        # Print the evaluation results
        print(f"Test Loss: {test_loss}")
        print(f"Test Accuracy: {test_accuracy}")
        print(f"Test Top K Accuracy: {test_top_k_accuracy}")

"""# Introduction
An introduction should introduce the problem you're working on, give some background and relevant detail for the reader, and explain why it is important.

# Analysis
Any exploratory analysis of your data, and general summarization of the data (e.g. summary statistics, correlation heatmaps, graphs, information about the data...). This can also include any cleaning and joining you did.

# Methods
Explain the structure of your model and your approach to building it. This can also include changes you made to your model in the process of building it. Someone should be able to read your methods section and *generally* be able to tell exactly what architechture you used.

# Results
Detailed discussion of how your models performed, including metrics, discussion of what metrics tell you about performance, any notable issues or patterns, and a comparison between models.

# Reflection
Reflections on what you learned/discovered in the process of doing the assignment. Things you would do differently in the future, ways you'll approach similar problems in the future, etc.

# What to Turn In

- PDF of your technical report
- your code as a .py, .ipynb, or link to github (you must turn it in either as a file, or a link to something that has timestamps of when the file was last edited)
- a README file as a .txt or .md
"""