# -*- coding: utf-8 -*-
"""Copy of HW3_SP24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DbZMv14JojEkuLj6vLHbsZ1mfVUZSjPL
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import os
import pathlib
import json


import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Corrected import for VGG16 and preprocess_input
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.regularizers import l2
from tensorflow.keras.losses import CategoricalCrossentropy

from tensorflow.keras.callbacks import EarlyStopping


os.environ["CUDA_VISIBLE_DEVICES"] = "7"
gpu_device = '/device:GPU:7'

train_dir = "archive/train"
test_dir = "archive/test"
val_dir = "archive/validation"

# Function to count the number of images in each category directory
# stuff for EDA
def count_images(directory):
    categories = []
    counts = []
    # Iterate through each category directory
    for category in sorted(os.listdir(directory)):
        # Full path to the category directory
        category_path = os.path.join(directory, category)
        # Check if it's a directory
        if os.path.isdir(category_path):
            # Append category name and count of images
            categories.append(category)
            counts.append(len(os.listdir(category_path)))
    return categories, counts

# Get counts for each dataset
train_categories, train_counts = count_images(train_dir)
val_categories, val_counts = count_images(val_dir)
test_categories, test_counts = count_images(test_dir)


def setup_data_generators(train_dir, val_dir, test_dir, batch_size=32):
    # Only augment training data with various image manipulation operations
    train_datagen = ImageDataGenerator(
        # Use VGG16's preprocess_input
        preprocessing_function=preprocess_input,
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    # The preprocess_input function standardizes images in a way that matches the original preprocessing applied to the images when the VGG16 model was trained.
    # This includes color channel normalization based on the means of the ImageNet dataset, which is crucial for models pre-trained on ImageNet.
    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical'
    )

    validation_generator = val_datagen.flow_from_directory(
        val_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical'
    )

    test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(224, 224),
        batch_size=batch_size,
        class_mode='categorical'
    )

    return train_generator, validation_generator, test_generator

with tf.device(gpu_device):
    train_generator, validation_generator, test_generator = setup_data_generators(train_dir, val_dir, test_dir)

# The number of classes would be the length of the class_indices dictionary
num_classes = len(train_generator.class_indices)

print(f"There are {num_classes} classes in the dataset.")

model = Sequential([
    # First convolutional block with input shape
    Conv2D(32, (3, 3), activation='relu', padding="same", input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),

    # Second convolutional block
    Conv2D(64, (3, 3), activation='relu', padding="same"),
    MaxPooling2D((2, 2)),

    # Third convolutional block
    Conv2D(128, (3, 3), activation='relu', padding="same"),
    MaxPooling2D((2, 2)),

    # Fourth convolutional block
    Conv2D(256, (3, 3), activation='relu', padding="same"),
    MaxPooling2D((2, 2)),

    # Fifth convolutional block
    Conv2D(512, (3, 3), activation='relu', padding="same"),
    MaxPooling2D((2, 2)),

    # Flattening the convolutions
    Flatten(),

    # Fully connected layers with Dropout as regularization
    Dense(512, activation='relu'),
    Dropout(0.5),

    # Output layer with softmax activation for classification
    Dense(36, activation='softmax'),
])

model.summary()

def save_model(model, export_dir):
    # Save the TensorFlow model in a Keras-compatible format
    model.save(export_dir)

    converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
    tflite_model = converter.convert()
    tflite_model_file = pathlib.Path('./a3_model.tflite')
    tflite_model_file.write_bytes(tflite_model)

def train_model():
    model.compile(optimizer = "adam",
                    loss='categorical_crossentropy',
                    metrics=['accuracy', 'top_k_categorical_accuracy'])

    # Set up EarlyStopping callback
    early_stopping = EarlyStopping(
        monitor = 'val_loss',
        patience = 10,
        restore_best_weights = True
    )

    # Train the model with the data generators and the EarlyStopping callback
    history = model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs = 200,
        callbacks = [early_stopping],
        verbose = 1
    )

    return history


with tf.device(gpu_device):
    history = train_model()

    history_path = 'history/training_history.json' 
    with open(history_path, 'w') as f:
        json.dump(history.history, f, indent=4)

    print(f"Training history saved to {history_path}")

    save_model(model, "a3_model")

    print("\nModel trained and saved.\n")

    # After training, evaluate the model on the test data
    test_loss, test_accuracy, test_top_k_accuracy = model.evaluate(
        test_generator,
        verbose=1
    )

    # Print the evaluation results
    print(f"Test Loss: {test_loss}")
    print(f"Test Accuracy: {test_accuracy}")
    print(f"Test Top K Accuracy: {test_top_k_accuracy}")
